# airflow-values.yaml

# Set the executor to KubernetesExecutor
# This will cause Airflow to spin up a new pod for each task instance.
executor: KubernetesExecutor

# Generate your own secret keys for production.
# Fernet Key: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# Webserver Key: openssl rand -hex 30
webserverSecretKey: "a745947747baf7bd0410bd526c88b3f1dc03a6b95cd1700baa992f73ed7f"
fernetKey: "Y1ToX3X8nztBJA56w9z5oCA1YImEn9PcR-vE3FZRIqU="

# Use git-sync to fetch DAGs from a remote repository.
# This is highly recommended for KubernetesExecutor.
dags:
  persistence:
    # We disable the PVC for dags because git-sync will handle them.
    enabled: false
  gitSync:
    enabled: false
    # Replace this with the URL to YOUR git repository containing your DAGs.
    repo: "https://github.com/apache/airflow.git"
    branch: "main"
    # Optional: if your DAGs are in a subdirectory of the repo.
    subPath: "airflow/example_dags"
    # For a private repo, you would configure credentials here using a secret.
    # sshKeySecret: my-ssh-key-secret
# It's good practice to set resource requests and limits for a real deployment
# but we'll use the defaults for this guide.
